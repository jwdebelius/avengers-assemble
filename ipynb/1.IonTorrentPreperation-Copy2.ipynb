{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "859fae62-1d76-4bbb-b37e-943e3fae6b90",
   "metadata": {},
   "source": [
    "**Author**: JW Debelius (justine.debelius@ki.se)<br>\n",
    "**Date**: September 2021<br>\n",
    "**Enviroment**: `qiime2-2021.8-dev`<br>\n",
    "**Python version**: 3.8<br>\n",
    "**Extra packages**: None<br>\n",
    "**QIIME version**: 2021.8-dev<br>\n",
    "**Extra Plugins**: q2-sidle (memory refactor); RESCRIPt (v. )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d8979-99df-43ba-9fb2-34431fbfb3c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mock Community Data Preperation\n",
    "\n",
    "## Background\n",
    "\n",
    "This notebook will process four mock community samples sequenced with the Ion torrent metagenomic kit, originally published by [Barb et al, 2015](https://pubmed.ncbi.nlm.nih.gov/26829716/). In the original paper, the authors profiled four mock communites using the 6 primer proprietary Ion Torrent kit. This kit using 6 primer pairs to target 7 regions along the 16Ss gene with both forward and reverse reads possible.\n",
    "* V2\n",
    "* V4\n",
    "* V8\n",
    "* V3\n",
    "* V67\n",
    "* V9\n",
    "\n",
    "In the original paper, the authors compared the performance for each region compared to the baseline. They clusstered sequences into de novo OTUs clustered with UPARSE; taxonomic assignment was made in QIIME 1 and then compared using abundance profiling, and deviation from published Shannon diversity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae5cab-c1b0-486d-a7f2-cf0d168cbe74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data download and avaliability\n",
    "\n",
    "Sequences were deposided in SRA under accession SUB1054354. We used the [SRA CLI tools]() to download the sequences and the provided sample sheet. Sequence fastq files and the description were saved in the `mock` folder in the `data/input` directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac87ff6-8a03-4739-8b4b-48b909d9de41",
   "metadata": {},
   "source": [
    "When they were deposited, the sequences were demultiplexed by sample, but not by region. The Ion Torrent kit produced reads for 12 regions (6forward and 6 reverse), meaning that to be able to use the sequences here, we need to split them into regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7066cd-56bb-4ba2-b97c-eda255b3a9c7",
   "metadata": {},
   "source": [
    "## Preprocessing Approach\n",
    "\n",
    "For the sake of not hating everyone and everything in existing, we will take a somewhat less optimal approach and try to do a regional demux on  already denoised sequences, because otherwise both I and my computer willcry.         \n",
    "So, Sequenced are prepared through the following steps: \n",
    "\n",
    "1. Sequences from each sample are filtered based on read length into batches based on read length. For this, we'll use 150, 200, 250, and 300 nt reads.\n",
    "2. Import reads for each length using the manifest format \n",
    "3. Denoise the per-region sequences to their fixed read length using dada2-pyro which better handles the error profile. Trim the reads to the approriate read length during sequencing. \n",
    "4. Use RESCRIPt to orient the sequences so they have a consistent orientation\n",
    "5. Align the reads against the reference sequences to seperate them into regions\n",
    "6. Identify he regions through some kind of witch craft\n",
    "7. Filter the sequences into samples and regions\n",
    "8. Account for all the sequences that were lost during filtering, aligning, \n",
    "<!-- 9. Consider life choices and decide why there is such an obsession with mock communitites -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fa58e-8dbf-436e-8246-9d063f794eb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4f60e5f-674c-4ec4-b3bf-e6f764bd0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import biom\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skbio\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from qiime2 import Artifact, Metadata, Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66101f5a-4c67-44de-bd5c-33b88db18277",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf5da3f-9e25-4e42-b304-fdc7c19517d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/inputs/mock/' # modify if you've placed the files in another location\n",
    "output_dir = 'data/output/mock/' # Change this if you want a different output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af0f1a9e-9ef0-477e-8276-75687700353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_lengths = np.array([200])\n",
    "# read_lengths = np.array([200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e38fc-ba0c-4cf5-9152-907dc4290812",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "104038f3-2fb4-498f-b14b-bcad005d46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = {\n",
    "    'split_by_length': {\n",
    "        'run': True,\n",
    "        'overwrite': True,\n",
    "        'input_dir': input_dir,\n",
    "        'output_dir': os.path.join(output_dir, '1.split_length')\n",
    "    },\n",
    "    'import_seqs': {\n",
    "        'run': True,\n",
    "        'overwrite': True,\n",
    "        'input_dir': os.path.join(output_dir, '1.split_length'),\n",
    "        'output_dir': os.path.join(output_dir, '2.split_manifest'),\n",
    "    },\n",
    "    'denoise_seqs': {\n",
    "        'run': True,\n",
    "        'overwrite': True,\n",
    "        'input_dir':  os.path.join(output_dir, '2.split_manifest'),\n",
    "        'output_dir':  os.path.join(output_dir, '3.denoised'),\n",
    "    },\n",
    "    'cluster_orientation': {\n",
    "        'run': True,\n",
    "        'overwrite': True,\n",
    "        'input_dir': os.path.join(output_dir, '3.denoised'),\n",
    "        'output_dir': os.path.join(output_dir, '4.split_orientation'),\n",
    "        'references': {'fwd': 'data/reference/gg_13_8_88/88_otus.qza',\n",
    "                       'rev': 'data/reference/gg_13_8_88/88_otus_rc.qza'},\n",
    "    },\n",
    "    'generate_alignment_reference': {\n",
    "        'run': True,\n",
    "        'input_refs': {\n",
    "            'fwd': 'data/reference/gg_13_8_88/gg_88_otus_aligned.qza',\n",
    "            'rev': 'data/reference/gg_13_8_88/gg_88_otus_aligned_rc.qza',\n",
    "            },\n",
    "        'taxonomy_fp': 'data/reference/gg_13_8_88/88_otu_taxonomy.qza',\n",
    "        'output_refs': {\n",
    "            'fwd': 'data/reference/gg_13_8_88/gg_13_8_aligned_enterobacteraceae_fwd.qza',\n",
    "            'rev': 'data/reference/gg_13_8_88/gg_13_8_aligned_enterobacteraceae_rev.qza',\n",
    "        },\n",
    "        'keep_group': 'f__Enterobacteriaceae',\n",
    "    },\n",
    "    'align_to_reference': {\n",
    "        'run': True,\n",
    "        'overwrite': True,\n",
    "        'input_dir':  os.path.join(output_dir, '4.split_orientation'),\n",
    "        'output_dir': os.path.join(output_dir, '5.aligned_to_ref'),\n",
    "        'reference_groups': 'f__Enterobacteriaceae',\n",
    "        'references': {'fwd':'data/reference/gg_13_8_88/gg_13_8_aligned_enterobacteraceae_fwd.qza',\n",
    "                       'rev': 'data/reference/gg_13_8_88/gg_13_8_aligned_enterobacteraceae_rev.qza',\n",
    "                       },\n",
    "    },\n",
    "    'extract_positions': {\n",
    "        'run': True,\n",
    "        'overwrite': True,\n",
    "        'input_align_dir': os.path.join(output_dir, '5.aligned_to_ref'),\n",
    "        'input_rep_seq_dir': os.path.join(output_dir,  '4.split_orientation'),\n",
    "        'input_table_dir': os.path.join(output_dir, '3.denoised'),\n",
    "        'output_dir': os.path.join(output_dir, '6.regional_identification'),\n",
    "    },\n",
    "    'split_to_region': {\n",
    "        'run': True,\n",
    "        'overwrite': True,\n",
    "        'input_map_dir': os.path.join(output_dir, '6.regional_identification'),\n",
    "        'input_data_dir': os.path.join(output_dir, '3.denoised'),\n",
    "        'output_dir': os.path.join(output_dir, '7.regional_demux'),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4725b26d-acbc-4aab-a5fd-aae3f1361db3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7b1c7c2-a331-4722-a880-78c70396fc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/reference/gg_13_8_88/gg_88_otus_aligned_rc.qza'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_ref = Artifact.load('data/reference/gg_13_8_88/gg_88_otus_aligned.qza').view(pd.Series)\n",
    "aligned_rev = aligned_ref.apply(lambda x: x.reverse_complement())\n",
    "aligned_rev = Artifact.import_data('FeatureData[AlignedSequence]', aligned_rev)\n",
    "aligned_rev.save('data/reference/gg_13_8_88/gg_88_otus_aligned_rc.qza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6633b57f-d314-4fff-bebe-ef36a07218dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<artifact: FeatureData[AlignedSequence] uuid: f87e33c2-8105-49ea-82db-e9d0afc26a99>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Artifact.load('data/reference/gg_13_8_88/gg_88_otus_aligned.qza')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a3648-eb6c-46c1-a5e2-15e8d93bacc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae0f2fe0-dbe6-4923-98e1-79c8c62a7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [fp.split('.')[0] for fp in os.listdir(input_dir)\n",
    "           if (os.path.splitext(fp)[1] == '.fastq') & ('_1' in fp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c636e32-ce4c-4496-84e6-e4f7e80d5b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SRR2182221_1', 'SRR2182220_1', 'SRR11180057_1', 'SRR2182222_1']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16fdf25-1032-4d79-9436-55ae352a0698",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "203d4dce-08c8-4320-829d-3d3f8f6fd7ad",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Get preprocessing references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a96ae-664a-4656-ae70-f9559b20fc4f",
   "metadata": {},
   "source": [
    "...|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988a7e7-7055-4958-ab99-473ea92c9c9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split sequences into read lengths\n",
    "\n",
    "We'll start by splitting sequences into batches based on the read lengths. For each sample,\n",
    "we'll load the sequences, determine the read lengths, and then group the sequences according to the read lengths provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64613137-5c1d-432b-9fc4-38266e86954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sequence_by_length(sample, ori_fastq, output_dir, \n",
    "                             read_lengths=read_lengths):\n",
    "    \"\"\"\n",
    "    Splits sequences in batches based on read on read lengths\n",
    "    \"\"\"\n",
    "    # Loads the file\n",
    "    seqs_with_qual = skbio.io.read(ori_fastq, \n",
    "                               format='fastq', \n",
    "                               phred_offset=33,\n",
    "                               )\n",
    "    # Determines the length and reads in the sequence\n",
    "    seq_lengths = pd.DataFrame.from_dict(orient='index', data={\n",
    "        seq.metadata['id']: {'seq': seq,\n",
    "                             'length': len(seq),\n",
    "                             }\n",
    "        for seq in seqs_with_qual\n",
    "    })\n",
    "    # Groups the ssequences into batches based on the read lengths\n",
    "    seq_group = pd.concat(axis=1, objs=[(seq_lengths['length'] > (length)) * 1 \n",
    "                          for length in read_lengths]).sum(axis=1)\n",
    "    seq_group.replace({i + 1: length for i, length in enumerate(read_lengths)},\n",
    "                       inplace=True)\n",
    "    seq_lengths['batch'] = seq_group\n",
    "    \n",
    "    seq_batches = \\\n",
    "        seq_lengths.groupby('batch', sort=False)['seq'].apply(lambda x: x.values)\n",
    "    seq_batches = seq_batches.loc[read_lengths]\n",
    "    \n",
    "    for length, reads in seq_batches.iteritems():\n",
    "        fp_ = f'{output_dir}/{length}.fastq'\n",
    "        f_ = skbio.io.open(fp_, 'w')\n",
    "        for seq in reads:\n",
    "            seq.write(f_, format='fastq', phred_offset=33)\n",
    "            \n",
    "    return seq_lengths[['length', 'batch']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb00352-bb8c-41eb-ac1b-90dd09c63d33",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80c32a-e2ef-4c4e-bb09-e4c2c96f884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if steps['split_by_length']['run']: \n",
    "\n",
    "#     step_overwrite = steps['split_by_length']['overwrite']\n",
    "#     step_output_dir = steps['split_by_length']['output_dir']\n",
    "\n",
    "#     os.makedirs(step_output_dir, exist_ok=step_overwrite)\n",
    "    \n",
    "#     seq_length_summary = dict()\n",
    "\n",
    "#     for sample in samples:\n",
    "#         print(sample)\n",
    "\n",
    "#         # Sets up the sample path and output directory\n",
    "#         ori_fastq = os.path.join(input_dir, f'{sample}.fastq')\n",
    "#         sample_dir = os.path.join(step_output_dir, sample)\n",
    "#         os.makedirs(sample_dir, exist_ok=step_overwrite)\n",
    "\n",
    "#         # Batches the sequences \n",
    "#         seq_lengths = \\\n",
    "#             batch_sequence_by_length(sample, ori_fastq, sample_dir, read_lengths)\n",
    "\n",
    "#         seq_length_summary[sample] = seq_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eac825-aa2e-4541-bc0a-52b2c7e4a2af",
   "metadata": {},
   "source": [
    "### Import the data into QIIME 2 using a manifest format\n",
    "\n",
    "Having split the data from each sample by read length, we'll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7dcdf85-6ee0-4e80-b097-097ca9bc9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_manifest(samples, read_length):\n",
    "    \"\"\"\n",
    "    Builds a sample manifest for a specified read length\n",
    "    \"\"\"\n",
    "    manifest = pd.DataFrame.from_dict(orient='index', data={\n",
    "        sample: {\n",
    "            'absolute-filepath': os.path.abspath(f'{manifest_input_dir}/'\n",
    "                                                 f'{sample}/{read_length}.fastq')\n",
    "        }\n",
    "        for sample in samples\n",
    "    })\n",
    "    manifest.index.set_names('sample-id', inplace=True)\n",
    "    return Metadata(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "619d023b-c4b0-40cd-9c71-d40d14de0bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 2891\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jusdeb/miniconda3/envs/qiime2-2021.8-dev/bin/qiime\", line 7, in <module>\n",
      "    from q2cli.__main__ import qiime\n",
      "  File \"/Users/jusdeb/miniconda3/envs/qiime2-2021.8-dev/lib/python3.8/site-packages/q2cli/__main__.py\", line 9, in <module>\n",
      "    import click\n",
      "  File \"/Users/jusdeb/miniconda3/envs/qiime2-2021.8-dev/lib/python3.8/site-packages/click/__init__.py\", line 7, in <module>\n",
      "    from .core import Argument\n",
      "  File \"/Users/jusdeb/miniconda3/envs/qiime2-2021.8-dev/lib/python3.8/site-packages/click/core.py\", line 2, in <module>\n",
      "    import inspect\n",
      "  File \"/Users/jusdeb/miniconda3/envs/qiime2-2021.8-dev/lib/python3.8/inspect.py\", line 40, in <module>\n",
      "    import linecache\n",
      "  File \"/Users/jusdeb/miniconda3/envs/qiime2-2021.8-dev/lib/python3.8/linecache.py\", line 11, in <module>\n",
      "    import tokenize\n",
      "  File \"/Users/jusdeb/miniconda3/envs/qiime2-2021.8-dev/lib/python3.8/tokenize.py\", line 45, in <module>\n",
      "    class TokenInfo(collections.namedtuple('TokenInfo', 'type string start end line')):\n",
      "  File \"/Users/jusdeb/miniconda3/envs/qiime2-2021.8-dev/lib/python3.8/collections/__init__.py\", line 394, in namedtuple\n",
      "    exec(s, namespace)\n",
      "  File \"<string>\", line 1, in <module>\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "\n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "if steps['import_seqs']['run']:\n",
    "    manifest_input_dir = step_output_dir\n",
    "    manifest_output_dir = steps['import_seqs']['output_dir']\n",
    "    manifest_overwrite = steps['import_seqs']['overwrite']\n",
    "\n",
    "    os.makedirs(manifest_output_dir, exist_ok=manifest_overwrite)\n",
    "\n",
    "    for read_length in read_lengths:\n",
    "        manifest_fp = f'{manifest_output_dir}/manifest_{read_length}.tsv'\n",
    "        seqs_art_fp = f'{manifest_output_dir}/demux_reads_{read_length}.qza'\n",
    "        seqs_vis_fp = f'{manifest_output_dir}/demux_reads_{read_length}.qzv'\n",
    "\n",
    "        manifest = build_manifest([samples[0]], read_length)\n",
    "        manifest.save(manifest_fp)\n",
    "\n",
    "        !qiime tools import \\\n",
    "          --type 'SampleData[SequencesWithQuality]' \\\n",
    "          --input-path $manifest_fp \\\n",
    "          --output-path $seqs_art_fp \\\n",
    "          --input-format SingleEndFastqManifestPhred33V2\n",
    "\n",
    "        !qiime demux summarize \\\n",
    "         --i-data $seqs_art_fp \\\n",
    "         --o-visualization $seqs_vis_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1904cc9a-ffd3-474e-be6a-475a8a23d5ed",
   "metadata": {},
   "source": [
    "### Denoise sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5008a6fd-846f-4231-8023-e3527b50588e",
   "metadata": {},
   "source": [
    "The recommendation for Ion Torrent sequencing is to denoise using dada2-denoise pyro **[citeation neededd]** so we'll follow that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd8d02-ebfe-4648-910f-c62ac258514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "Aborted!\n",
      "\u001b[32mSaved Visualization to: data/output/mock/3.denoised/denosing_stats_200.qzv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if steps['denoise_seqs']['run']:\n",
    "    denoised_input = steps['denoise_seqs']['input_dir']\n",
    "    denoised_output = steps['denoise_seqs']['output_dir']\n",
    "    denoised_overwrite = steps['denoise_seqs']['overwrite']\n",
    "    os.makedirs(denoised_output, exist_ok=denoised_overwrite)\n",
    "\n",
    "    for read_length in read_lengths:\n",
    "        seqs_art_fp = f'{denoised_input}/demux_reads_{read_length}.qza'\n",
    "        table_art_fp = f'{denoised_output}/table_{read_length}.qza'\n",
    "        rep_seq_art_fp = f'{denoised_output}/rep_seq_{read_length}.qza'\n",
    "        stats_art_fp = f'{denoised_output}/denosing_stats_{read_length}.qza'\n",
    "        table_viz_fp = f'{denoised_output}/table_{read_length}.qzv'\n",
    "        stats_viz_fp = f'{denoised_output}/denosing_stats_{read_length}.qzv'\n",
    "\n",
    "        !qiime dada2 denoise-pyro \\\n",
    "         --i-demultiplexed-seqs $seqs_art_fp \\\n",
    "         --p-trunc-len $read_length \\\n",
    "         --p-hashed-feature-ids \\\n",
    "         --o-table $table_art_fp \\\n",
    "         --o-representative-sequences $rep_seq_art_fp \\\n",
    "         --o-denoising-stats $stats_art_fp\n",
    "\n",
    "        !qiime metadata tabulate \\\n",
    "         --m-input-file $stats_art_fp \\\n",
    "         --o-visualization $stats_viz_fp\n",
    "\n",
    "        !qiime feature-table summarize \\\n",
    "         --i-table $table_art_fp \\\n",
    "         --o-visualization $table_viz_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf842b29-e465-4f8c-bed6-a6a7912dc23b",
   "metadata": {},
   "source": [
    "And now we have a set of full denoise tables with a fixed read lenght and mixed orientation and region reads. These are now ready for demultiplexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf5cc8-69fc-4f8f-b9ed-141ed529eb42",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regional Demultiplexing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534ccb0-7c8f-42da-905a-ae1ecea67508",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split the data into forward and reverse reads\n",
    "\n",
    "We'll split the data by orientation using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc652fea-da34-47fc-bde6-71edfdde47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if steps['cluster_orientation']['run']:\n",
    "    orient_input_dir = steps['cluster_orientation']['input_dir']\n",
    "    orient_output_dir = steps['cluster_orientation']['output_dir']\n",
    "    orient_overwrite = steps['cluster_orientation']['overwrite']\n",
    "    references =  steps['cluster_orientation']['references']\n",
    "\n",
    "    os.makedirs(orient_output_dir, exist_ok=orient_overwrite)\n",
    "\n",
    "    flip_dir = {\"fwd\": 'rev', 'rev': 'fwd'}\n",
    "\n",
    "    for read_length in read_lengths:\n",
    "        input_seqs = f'{orient_input_dir}/rep_seq_{read_length}.qza'\n",
    "        input_table = f'{orient_input_dir}/table_{read_length}.qza'\n",
    "\n",
    "        for dir_, ref_fp in references.items():\n",
    "            dir2 = flip_dir[dir_]\n",
    "            cluster_fp = f'{orient_output_dir}/clustered_{read_length}_{dir_}.qza'\n",
    "            table_fp = f'{orient_output_dir}/table_{read_length}_{dir_}.qza'\n",
    "            discard_fp = f'{orient_output_dir}/discard_{read_length}_{dir2}.qza'\n",
    "\n",
    "            !qiime vsearch cluster-features-closed-reference \\\n",
    "             --i-sequences $input_seqs \\\n",
    "             --i-table $input_table \\\n",
    "             --i-reference-sequences $ref_fp \\\n",
    "             --p-perc-identity 0.85 \\\n",
    "             --p-strand plus \\\n",
    "             --o-clustered-table $table_fp \\\n",
    "             --o-clustered-sequences $cluster_fp \\\n",
    "             --o-unmatched-sequences $discard_fp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18ae194-76f9-440e-996f-ddcd19f2fa50",
   "metadata": {},
   "source": [
    "### Generate sub alignment references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b45f7-75c6-48ad-b3d9-ac27628eb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if steps['generate_alignment_reference']['run']: \n",
    "    input_refs = steps['generate_alignment_reference']['input_refs']\n",
    "    output_refs = steps['generate_alignment_reference']['output_refs']\n",
    "    taxonomy_fp = steps['generate_alignment_reference']['taxonomy_fp']\n",
    "    keep_group = steps['generate_alignment_reference']['keep_group']\n",
    "\n",
    "    for dir_, input_ in input_refs.items():\n",
    "        output = output_refs[dir_]\n",
    "        if not os.path.exists(output):\n",
    "            !qiime taxa filter-seqs \\\n",
    "             --i-sequences $input_ \\\n",
    "             --i-taxonomy $taxonomy_fp \\\n",
    "             --p-include $keep_group \\\n",
    "             --o-filtered-sequences $output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082cd7f2-52bb-4794-af2a-67f56e0a475e",
   "metadata": {},
   "source": [
    "### Align the oriented reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2de1f-ee8a-4e32-8ae1-7bbf7dbe0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if steps['align_to_reference']['run']: \n",
    "    align_input_dir = steps['align_to_reference']['input_dir']\n",
    "    align_output_dir = steps['align_to_reference']['output_dir']\n",
    "    align_overwrite = steps['align_to_reference']['overwrite']\n",
    "    align_references =  steps['align_to_reference']['references']\n",
    "\n",
    "    os.makedirs(align_output_dir, exist_ok=align_overwrite)\n",
    "\n",
    "    for length in read_lengths:\n",
    "        for dir_, ref_alignment_fp in align_references.items():\n",
    "            rep_seq_fp = f'{align_input_dir}/discard_{read_length}_{dir_}.qza'\n",
    "            aligned_output = (f'{align_output_dir}/repset_aligned._{length}_'\n",
    "                              f'{dir_}.qza')\n",
    "            !qiime alignment mafft-add \\\n",
    "             --i-alignment $ref_alignment_fp \\\n",
    "             --i-sequences $rep_seq_fp \\\n",
    "             --o-expanded-alignment $aligned_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b6172-c8f7-44a0-b11a-f31fa1fea7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!qiime alignment mafft-add --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf4656-a52c-49e3-a8a7-76df5c562c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "align_references\n",
    "# seqs = Artifact.load('data/reference/gg_13_8_88/gg_13_8_aligned_enterobacteraceae_fwd.qza').view(DN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776fcf2-d26f-4350-84f7-773dcebeb9ff",
   "metadata": {},
   "source": [
    "### Extracts the starting position from the alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487030b-852a-4149-ad48-02f5c0350ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if steps['extract_positions']['run']:\n",
    "    first_pos_align_dir = steps['extract_positions']['input_align_dir']\n",
    "    first_pos_repseq_dir = steps['extract_positions']['input_rep_seq_dir']\n",
    "    first_pos_table_dir_ = steps['extract_positions']['input_table_dir']\n",
    "    first_pos_output_dir =  steps['extract_positions']['output_dir']\n",
    "    first_pos_ovewrite = steps['extract_positions']['overwrite']\n",
    "\n",
    "    os.makedirs(first_pos_output_dir, exist_ok=first_pos_ovewrite)\n",
    "\n",
    "    for length in read_lengths:\n",
    "        for dir_ in ['fwd', 'rev']:\n",
    "            alignment_fp = \\\n",
    "                f'{first_pos_align_dir}/repset_aligned._{length}_{dir_}.qza'\n",
    "            rep_seq_fp = \\\n",
    "                f'{first_pos_repseq_dir}/discard_{read_length}_{dir_}.qza'\n",
    "            table_fp = f'{first_pos_table_dir_}/table_{read_length}.qza'\n",
    "            output_pos_art = \\\n",
    "                f'{first_pos_output_dir}/starts-{length}-{dir_}.qza'\n",
    "            output_pos_viz = \\\n",
    "                f'{first_pos_output_dir}/starts-{length}-{dir_}.qzv'\n",
    "\n",
    "            !qiime sidle find-first-alignment-position \\\n",
    "             --i-alignment $alignment_fp \\\n",
    "             --i-representative-sequences $rep_seq_fp \\\n",
    "             --i-table $table_fp \\\n",
    "             --p-direction $dir_ \\\n",
    "             --o-position-summary $output_pos_art \n",
    "\n",
    "            !qiime sidle summarize-alignment-positions \\\n",
    "              --i-alignment $alignment_fp \\\n",
    "              --i-position-summary $output_pos_art \\\n",
    "              --p-sort-cols 'starting-position,sequence-counts' \\\n",
    "              --p-weight-by-abundance \\\n",
    "              --p-colormap 'viridis' \\\n",
    "              --p-heatmap-maskcolor 'k' \\\n",
    "              --o-visualization $output_pos_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90be7770-f491-4c07-9e64-4d5ceb525e2e",
   "metadata": {},
   "source": [
    "### Regional Demultiplexing \n",
    "\n",
    "Based on the visualizaation, I can infer a set of starting positions. For the forward reads, I looked at feature with at least 2 sequences and a maxium relative abundnce of at least 1000 sequences. This gives me starting at 69, 303, 508, 914, 1043, 1285 for the forward reads. The reverse positions are a little harder because of the weird split in thata block around 400. But, we also read the starting position backward, so maybe it's not so weird? The reverse reads end up at 349, 534, 805, 1134, 1302, and 1455."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad6dcb-c28d-4a62-8f9f-a2d4446d1840",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_positions = {'fwd': [ 69, 303, 508,  914, 1043, 1285],\n",
    "                   'rev': [349, 534, 805, 1143, 1302, 1455]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee65714-3932-4d33-ad86-609e790b4fda",
   "metadata": {},
   "source": [
    "I'll use those positions to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8bdc6-8743-43cd-bbed-59ede5ced3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if steps['split_to_region']['run']: \n",
    "    region_demux_overwrite = steps['split_to_region']['overwrite']\n",
    "    region_demux_meta_dir = steps['split_to_region']['input_map_dir']\n",
    "    region_demux_data_dir = steps['split_to_region']['input_data_dir']\n",
    "    region_demux_ouput_dir = steps['split_to_region']['output_dir']\n",
    "\n",
    "    os.makedirs(region_demux_ouput_dir, exist_ok=region_demux_overwrite)\n",
    "\n",
    "    for length in read_lengths:\n",
    "        for dir_, positions in first_positions.items():\n",
    "            for pos in positions:\n",
    "                input_table_fp = f'{region_demux_data_dir}/table_{length}.qza'\n",
    "                input_rep_seq_fp = \\\n",
    "                    f'{region_demux_data_dir}/rep_seq_{length}.qza'\n",
    "\n",
    "                meta_fp = f'{region_demux_meta_dir}/starts-{length}-{dir_}.qza'\n",
    "\n",
    "                table_fp = \\\n",
    "                    f'{region_demux_ouput_dir}/table-{length}-{dir_}-{pos}.qza'\n",
    "                rep_seq_fp = \\\n",
    "                    f'{region_demux_ouput_dir}/rep-seq-{length}-{dir_}-{pos}.qza'\n",
    "                table_summary_fp = \\\n",
    "                     f'{region_demux_ouput_dir}/table-{length}-{dir_}-{pos}.qzv'\n",
    "\n",
    "                where = f'[starting-position]=\"{pos}\"'\n",
    "\n",
    "                !qiime feature-table filter-features \\\n",
    "                 --i-table $input_table_fp \\\n",
    "                 --m-metadata-file $meta_fp \\\n",
    "                 --p-where $where \\\n",
    "                 --o-filtered-table $table_fp\n",
    "\n",
    "                !qiime feature-table filter-seqs \\\n",
    "                 --i-data $input_rep_seq_fp \\\n",
    "                 --i-table $table_fp \\\n",
    "                 --o-filtered-data $rep_seq_fp\n",
    "                \n",
    "                !qiime feature-table summarize \\\n",
    "                 --i-table $table_fp \\\n",
    "                 --o-visualization $table_summary_fp\n",
    "                \n",
    "                if dir_ == 'rev':\n",
    "                    !qiime sidle reverse-complement-sequence \\\n",
    "                     --i-sequence $rep_seq_fp \\\n",
    "                     --o-reverse-complement $rep_seq_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d8c648-1eaa-4b0f-8b23-cac0e3c6a95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20165a59-0a85-45b9-843e-29378c8c125b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Accounting\n",
    "\n",
    "Finally, I'd like to determine how many sequences were lost and where they were lost. For this, I need the dada2 stats... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dadf8055-1bd2-42cc-a971-564c0cb1b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_counts = pd.DataFrame({\n",
    "    sample: counts['batch'].value_counts()\n",
    "    for sample, counts in seq_length_summary.items()\n",
    "})\n",
    "read_counts.index.set_names('read_length', inplace=True)\n",
    "read_counts.columns.set_names('sample-id', inplace=True)\n",
    "read_counts = read_counts.T\n",
    "read_counts\n",
    "\n",
    "dropped_reads = read_counts.drop(columns=read_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d0d3933a-c926-47fb-bfff-f38888696e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dada2_summaries = {\n",
    "    length:  Artifact.load(f'data/output/mock/3.denoised/denosing_stats_{length}.qza')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2bb233-3448-447f-b243-0b7e517d3553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6523833c-6c15-49ec-86e0-8a64603a2007",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reconstruction\n",
    "\n",
    "## Database\n",
    "\n",
    "To extract the positions, I'm going to use the most abundant features from each region and align them against the full reference set. For this analysis, I want to work with Silva 128 [cite]. I picked 128 specifically to be able to do phylogenetic tree reconstruction, although at this point, the phylogenny doesns't really matter, so I guess it's more for consistency with the simulated data ¯\\\\\\_(ツ)\\_/¯. I think we've demonstrated the use o fmultiple databases sucessfully, if not I can switch this to greengenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5892c7c2-79b5-4863-bc78-7c068b61bb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dir = 'data/reference/silva/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76272a07-c5b3-433c-bf77-9e83aaa74557",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_alignment = os.path.join(reference_dir, 'silva-128-99-aligned-seqs.qza')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a5cf67-8845-4359-b4e7-0055b57ef240",
   "metadata": {},
   "source": [
    "# Filters the reference alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ec8311-2a28-445a-9e63-e94e0999192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: \u001b[34mqiime rescript\u001b[0m [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Description: Reference sequence annotation and curation pipeline.\n",
      "\n",
      "  Plugin website: https://github.com/nbokulich/RESCRIPt\n",
      "\n",
      "  Getting user support: Please post to the QIIME 2 forum for help with this\n",
      "  plugin: https://forum.qiime2.org\n",
      "\n",
      "\u001b[1mOptions\u001b[0m:\n",
      "  \u001b[34m--version\u001b[0m    Show the version and exit.\n",
      "  \u001b[34m--citations\u001b[0m  Show citations and exit.\n",
      "  \u001b[34m--help\u001b[0m       Show this message and exit.\n",
      "\n",
      "\u001b[1mCommands\u001b[0m:\n",
      "  \u001b[34mcull-seqs\u001b[0m                    Removes sequences that contain at least the\n",
      "                               specified number of degenerate bases and/or\n",
      "                               homopolymers of a given length.\n",
      "\n",
      "  \u001b[34mdegap-seqs\u001b[0m                   Remove gaps from DNA sequence alignments.\n",
      "  \u001b[34mdereplicate\u001b[0m                  Dereplicate features with matching sequences\n",
      "                               and taxonomies.\n",
      "\n",
      "  \u001b[34medit-taxonomy\u001b[0m                Edit taxonomy strings with find and replace\n",
      "                               terms.\n",
      "\n",
      "  \u001b[34mevaluate-classifications\u001b[0m     Interactively evaluate taxonomic classification\n",
      "                               accuracy.\n",
      "\n",
      "  \u001b[34mevaluate-cross-validate\u001b[0m      Evaluate DNA sequence reference database via\n",
      "                               cross-validated taxonomic classification.\n",
      "\n",
      "  \u001b[34mevaluate-fit-classifier\u001b[0m      Evaluate and train naive Bayes classifier on\n",
      "                               reference sequences.\n",
      "\n",
      "  \u001b[34mevaluate-seqs\u001b[0m                Compute summary statistics on sequence\n",
      "                               artifact(s).\n",
      "\n",
      "  \u001b[34mevaluate-taxonomy\u001b[0m            Compute summary statistics on taxonomy\n",
      "                               artifact(s).\n",
      "\n",
      "  \u001b[34mfilter-seqs-length\u001b[0m           Filter sequences by length.\n",
      "  \u001b[34mfilter-seqs-length-by-taxon\u001b[0m  Filter sequences by length and taxonomic group.\n",
      "  \u001b[34mfilter-taxa\u001b[0m                  Filter taxonomy by list of IDs or search\n",
      "                               criteria.\n",
      "\n",
      "  \u001b[34mget-ncbi-data\u001b[0m                Download, parse, and import NCBI sequences and\n",
      "                               taxonomies\n",
      "\n",
      "  \u001b[34mget-ncbi-data-protein\u001b[0m        Download, parse, and import NCBI protein\n",
      "                               sequences and taxonomies\n",
      "\n",
      "  \u001b[34mget-silva-data\u001b[0m               Download, parse, and import SILVA database.\n",
      "  \u001b[34mmerge-taxa\u001b[0m                   Compare taxonomies and select the longest,\n",
      "                               highest scoring, or find the least common\n",
      "                               ancestor.\n",
      "\n",
      "  \u001b[34morient-seqs\u001b[0m                  Orient input sequences by comparison against\n",
      "                               reference.\n",
      "\n",
      "  \u001b[34mparse-silva-taxonomy\u001b[0m         Generates a SILVA fixed-rank taxonomy.\n",
      "  \u001b[34mreverse-transcribe\u001b[0m           Reverse transcribe RNA to DNA sequences.\n",
      "  \u001b[34msubsample-fasta\u001b[0m              Subsample an indicated number of sequences from\n",
      "                               a FASTA file.\n",
      "\n",
      "  \u001b[34mtrim-alignment\u001b[0m               Trim alignment based on provided primers or\n",
      "                               specific positions.\n"
     ]
    }
   ],
   "source": [
    "!qiime rescript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219c7e4-c2a5-4065-a7bf-9fcc1ac2f9b0",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c0ab4e6-6de1-47be-8964-49d1f48ccb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = 'fwd'\n",
    "pos = 303\n",
    "thresh = 1000\n",
    "length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "531c6cad-dd8b-4402-ba87-ec06a3d2b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feat_output_dir = 'data/output/mock/8.top-features'\n",
    "top_feat_input_dir = 'data/output/mock/7.regional_demux/'\n",
    "os.makedirs(top_feat_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d373c01-3518-4ba6-a2e1-e744de345972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSaved FeatureTable[Frequency] to: data/output/mock/8.top-features/table-200-fwd-303-1000.qza\u001b[0m\n",
      "\u001b[32mSaved FeatureData[Sequence] to: data/output/mock/8.top-features/rep-seq-200-fwd-303-1000.qza\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "full_table_fp = f'{top_feat_input_dir}/table-{length}-{dir_}-{pos}.qza'\n",
    "full_repseq_fp = f'{top_feat_input_dir}/rep-seq-{length}-{dir_}-{pos}.qza'\n",
    "filt_table_fp = f'{top_feat_output_dir}/table-{length}-{dir_}-{pos}-{thresh}.qza'\n",
    "filt_repseq_fp = f'{top_feat_output_dir}/rep-seq-{length}-{dir_}-{pos}-{thresh}.qza'\n",
    "\n",
    "!qiime feature-table filter-features \\\n",
    " --i-table $full_table_fp \\\n",
    " --p-min-frequency $thresh \\\n",
    " --o-filtered-table $filt_table_fp\n",
    "\n",
    "!qiime feature-table filter-seqs \\\n",
    " --i-data $full_repseq_fp \\\n",
    " --i-table $filt_table_fp \\\n",
    " --o-filtered-data $filt_repseq_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b759dd-d84d-4e4b-952b-69ac766b3711",
   "metadata": {},
   "source": [
    "Aligns the filtered sequences against the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a5f358-c9ad-4f4c-a4dc-b876d0d005a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5455b69d-77e1-4f88-aa02-7a3308d5520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mPlugin error from alignment:\n",
      "\n",
      "  Command '['mafft', '--preservecase', '--inputorder', '--thread', '1', '--addfragments', '/var/folders/bw/q064ds0d2795_6mxnrssf0l1gkw0rj/T/qiime2-archive-0s3ez_d2/de0bd300-82bc-45be-81fc-fb47108111c4/data/dna-sequences.fasta', '/var/folders/bw/q064ds0d2795_6mxnrssf0l1gkw0rj/T/qiime2-archive-2hhw1_ya/20a7cc70-0cff-46b3-a0a1-606dbd5b3147/data/aligned-dna-sequences.fasta']' returned non-zero exit status 1.\n",
      "\n",
      "Debug info has been saved to /var/folders/bw/q064ds0d2795_6mxnrssf0l1gkw0rj/T/qiime2-q2cli-err-3sfvad0e.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!qiime alignment mafft-add \\\n",
    " --i-alignment $reference_alignment \\\n",
    " --i-sequences  $filt_repseq_fp \\\n",
    " --p-addfragments \\\n",
    " --o-expanded-alignment $top_feat_output_dir/silva-128-99-aligned-200-fwd-300-100-add.qza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465405ef-0a79-4f45-81e9-ad2c30db579a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!qiime alignment mafft-add \\\n",
    " --i-alignment $reference_alignment \\\n",
    " --i-sequences  $filt_repseq_fp \\\n",
    " --p-no-addfragments \\\n",
    " --o-expanded-alignment $top_feat_output_dir/silva-128-99-aligned-200-fwd-300-100-no-add.qza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c5f212-aa95-43ec-9508-cdbfe5530fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: \u001b[34mqiime rescript subsample-fasta\u001b[0m [OPTIONS]\n",
      "\n",
      "  Subsample a set of sequences (either plain or aligned DNA)based on a\n",
      "  fraction of original sequences.\n",
      "\n",
      "\u001b[1mInputs\u001b[0m:\n",
      "  \u001b[34m\u001b[4m--i-sequences\u001b[0m ARTIFACT \u001b[32mFeatureData[AlignedSequence¹ | Sequence²]\u001b[0m\n",
      "                          Sequences to subsample from.              \u001b[35m[required]\u001b[0m\n",
      "\u001b[1mParameters\u001b[0m:\n",
      "  \u001b[34m--p-subsample-size\u001b[0m PROPORTION \u001b[32mRange(0, 1, inclusive_start=False,\u001b[0m\n",
      "    \u001b[32minclusive_end=True)\u001b[0m   Size of the random sample as a fraction of the\n",
      "                          total count                           \u001b[35m[default: 0.1]\u001b[0m\n",
      "  \u001b[34m--p-random-seed\u001b[0m INTEGER Seed to be used for random sampling.\n",
      "    \u001b[32mRange(1, None)\u001b[0m                                                \u001b[35m[default: 1]\u001b[0m\n",
      "\u001b[1mOutputs\u001b[0m:\n",
      "  \u001b[34m\u001b[4m--o-sample-sequences\u001b[0m ARTIFACT \u001b[32mFeatureData[AlignedSequence¹ | Sequence²]\u001b[0m\n",
      "                          Sample of original sequences.             \u001b[35m[required]\u001b[0m\n",
      "\u001b[1mMiscellaneous\u001b[0m:\n",
      "  \u001b[34m--output-dir\u001b[0m PATH       Output unspecified results to a directory\n",
      "  \u001b[34m--verbose\u001b[0m / \u001b[34m--quiet\u001b[0m     Display verbose output to stdout and/or stderr\n",
      "                          during execution of this action. Or silence output\n",
      "                          if execution is successful (silence is golden).\n",
      "  \u001b[34m--examples\u001b[0m              Show usage examples and exit.\n",
      "  \u001b[34m--citations\u001b[0m             Show citations and exit.\n",
      "  \u001b[34m--help\u001b[0m                  Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!qiime rescript subsample-fasta --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241ba94-22f2-4794-af79-9e541abdc15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
